"""
Analytics & Evaluation Agent - Monitors performance and generates insights.

Tracks campaign performance, evaluates metrics, generates reports,
and provides data-driven insights for optimization.
"""

from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta, timezone
import logging
import statistics
from scipy import stats
import numpy as np

from ..core.base_agent import BaseAgent, AgentStatus
from ..core.handoff_detector import HandoffDetector
from ..tools.metrics_calculator import (
    calculate_campaign_metrics,
    calculate_agent_metrics,
    calculate_system_metrics,
    calculate_time_series_metrics,
    calculate_comparison_metrics,
)
from ..tools.visualization import (
    generate_campaign_dashboard,
    generate_agent_dashboard,
    generate_system_dashboard,
    create_line_chart,
    create_funnel_chart,
)
from ..tools.synthetic_data_loader import load_execution_data


class AnalyticsEvaluationAgent(BaseAgent):
    """Specialized agent for analytics and performance evaluation."""

    def __init__(
        self,
        config: Dict[str, Any],
        memory_manager=None,
        message_bus=None,
        prompt_manager=None,
    ):
        """Initialize analytics agent."""
        super().__init__(
            agent_id="analytics_evaluation",
            name="Analytics Evaluation Agent",
            description="Monitors performance and generates insights",
            config=config,
            memory_manager=memory_manager,
            message_bus=message_bus,
            prompt_manager=prompt_manager,
        )
        self.metrics_history: List[Dict] = []
        self.kpi_definitions = {}
        # Initialize LLM-driven handoff detector
        handoff_config = self.config.get("agents", {}).get("orchestrator", {}).get("handoff_detector")
        self.handoff_detector = HandoffDetector(llm=self.llm, config=handoff_config)

    def should_handoff(self, context: Dict[str, Any]) -> Optional[Any]:
        """
        Override base handoff logic for explicit handoff control.

        Only handoff when the LLM explicitly requests it by setting handoff flags.
        This prevents automatic handoffs from keyword matching in analytics results.

        Args:
            context: Current execution context (typically the result)

        Returns:
            HandoffRequest if explicit handoff requested, None otherwise
        """
        # Check if this is an error case that needs escalation
        if context.get("success") is False and context.get("error"):
            # Even on errors, mark as final to avoid loops
            return None

        # Check if result explicitly requests a handoff
        if context.get("handoff_required") is True:
            target_agent = context.get("target_agent")
            reason = context.get("handoff_reason", "explicit_handoff_request")

            if target_agent:
                from src.marketing_agents.core.base_agent import HandoffRequest

                self.logger.info(
                    f"Explicit handoff requested: {self.agent_id} -> {target_agent}"
                )
                return HandoffRequest(
                    from_agent=self.agent_id,
                    to_agent=target_agent,
                    reason=reason,
                    context=context,
                )

        # For normal analytics reports, do not handoff
        # The agent should complete its analysis and return final results
        return None

    def _register_tools(self) -> None:
        """Register analytics-specific tools."""
        self.register_tool("calculate_metrics", self._calculate_metrics_tool)
        self.register_tool("generate_report", self._generate_report_tool)
        self.register_tool("forecast_performance", self._forecast_tool)
        self.register_tool("detect_anomalies", self._detect_anomalies_tool)
        self.logger.info(f"Registered {len(self.tools)} tools for analytics agent")

    async def _calculate_metrics_tool(
        self, time_range: str = "24h", metric_types: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Tool wrapper for metrics calculation."""
        return self._calculate_metrics(time_range, metric_types)

    async def _generate_report_tool(self, metrics: Dict[str, Any]) -> str:
        """Tool wrapper for report generation."""
        return self._generate_report(metrics)

    async def _forecast_tool(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Tool wrapper for forecasting."""
        return {"forecast": "Forecast data", "confidence": 0.85}

    async def _detect_anomalies_tool(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Tool wrapper for anomaly detection."""
        return {"anomalies": [], "threshold": 0.95}

    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Process analytics request."""
        self.status = AgentStatus.PROCESSING
        start_time = datetime.utcnow()

        try:
            # Log input_data for debugging
            self.logger.info(
                f"Analytics agent received input_data type: {type(input_data)}"
            )
            self.logger.info(f"Analytics agent received input_data: {input_data}")

            # Handle case where input_data might be None
            if input_data is None:
                self.logger.error("input_data is None!")
                return {
                    "success": False,
                    "error": "input_data is None",
                    "timestamp": datetime.utcnow().isoformat(),
                    "is_final": True,
                    "summary": "Analytics processing failed: input_data is None",
                }

            elif not isinstance(input_data, dict):
                input_data = {
                    "type": "performance_report",
                    "raw_input": str(input_data),
                }

            # Normalize API request format to internal format
            # API sends: report_type, date_range, message
            # Internal expects: type, time_range, message
            if "report_type" in input_data:
                # Map report_type to type
                input_data["type"] = input_data.get("report_type", "performance_report")

            request_type = input_data.get("type", "performance_report")

            # Extract user query from filters or message
            user_query = input_data.get("message", "")
            if not user_query:
                filters = input_data.get("filters")
                if filters and isinstance(filters, dict):
                    user_query = filters.get("user_query", "")

            # Store user query for handoff detection
            if user_query:
                input_data["message"] = user_query
                self.logger.info(f"ðŸ“ User query extracted: '{user_query}'")
            else:
                self.logger.warning("âš ï¸ No user query found in input_data")

            self.logger.info(f"Processing analytics request: {request_type}")

            # Route to appropriate handler
            if request_type == "calculate_metrics":
                result = await self._handle_calculate_metrics(input_data)
            elif request_type == "generate_report":
                result = await self._handle_generate_report(input_data)
            elif request_type == "forecast_performance":
                result = await self._handle_forecast(input_data)
            elif request_type == "detect_anomalies":
                result = await self._handle_anomaly_detection(input_data)
            elif request_type == "analyze_ab_test":
                result = await self._handle_ab_test(input_data)
            else:
                # Default: comprehensive performance report
                # Extract time_range from input_data or date_range
                # Default to 365d to capture synthetic historical data
                time_range = input_data.get("time_range", "365d")

                # If date_range is provided (from API), ensure we use a long enough range
                if "date_range" in input_data:
                    # Use a longer time range to capture all synthetic data
                    if time_range in ["24h", "7d", "30d"]:
                        time_range = "365d"
                    self.logger.info(
                        f"Using date_range, setting time_range to {time_range}"
                    )

                metrics = self._calculate_metrics(time_range=time_range)

                # Generate contextual report based on user query
                if user_query:
                    report = self._generate_contextual_report(metrics, user_query)
                    self.logger.info(
                        f"ðŸ“ DEBUG: Generated report type: {type(report).__name__}, length: {len(report) if isinstance(report, str) else 'N/A'}"
                    )
                    if isinstance(report, str) and len(report) > 0:
                        self.logger.info(
                            f"ðŸ“ DEBUG: First 200 chars of report: {report[:200]}"
                        )
                else:
                    report = self._generate_report(metrics)

                result = {
                    "request_type": request_type,
                    "metrics": metrics,
                    "report": report,
                    "user_query": user_query,
                }

                # Log for debugging
                self.logger.info(
                    f"âœ… Generated analytics result with report type: {type(report).__name__}"
                )

            # Ensure result is not None
            if result is None:
                self.logger.error(
                    f"Handler returned None for request_type: {request_type}"
                )
                result = {
                    "request_type": request_type,
                    "metrics": {},
                    "report": "Error: No data available",
                    "error": "Handler returned None",
                }

            self.status = AgentStatus.IDLE

            # Check if this is a handoff from another agent
            # If so, DON'T check for further handoffs to prevent infinite loops
            is_handoff_result = input_data.get("from_agent") is not None

            # Also check handoff history to detect loops
            handoff_history = input_data.get("handoff_history", [])
            recent_handoffs = [
                (h["from"], h["to"]) for h in handoff_history[-4:]
            ]  # Last 4 handoffs

            # Count how many times we've seen analytics -> marketing -> analytics pattern
            loop_count = sum(
                1
                for i in range(len(recent_handoffs) - 1)
                if recent_handoffs[i] == ("analytics_evaluation", "marketing_strategy")
                and recent_handoffs[i + 1]
                == ("marketing_strategy", "analytics_evaluation")
            )

            # Check if handoff is needed based on analysis results
            # But skip if we're already handling a handoff result OR if we detect a loop
            if is_handoff_result or loop_count >= 1:
                if is_handoff_result:
                    self.logger.info(
                        f"Processing handoff from {input_data.get('from_agent')}. "
                        "Completing analysis without further handoffs."
                    )
                elif loop_count >= 1:
                    self.logger.warning(
                        f"Detected handoff loop (count={loop_count}). "
                        "Completing analysis without further handoffs to prevent infinite loop."
                    )
                handoff_info = {}
            else:
                self.logger.info(
                    f"ðŸ” Checking handoff need for query: '{user_query[:100] if user_query else 'No query'}'"
                )
                handoff_info = await self._detect_handoff_need(result, input_data)
                if handoff_info.get("handoff_required"):
                    self.logger.info(
                        f"âœ¨ Handoff detected â†’ {handoff_info.get('target_agent')} (reason: {handoff_info.get('handoff_reason')})"
                    )
                else:
                    self.logger.info("â¹ï¸ No handoff required - completing analysis")

            # Generate meaningful summary based on user query or handoff
            if handoff_info.get("handoff_required"):
                target = handoff_info.get("target_agent", "")
                summary = f"Routing to {target} agent for specialized assistance."
            elif user_query:
                summary = f"Analytics report for: {user_query[:80]}..."
            else:
                summary = f"Analytics report generated: {request_type}"

            response = {
                "success": True,
                "analytics": result,
                "timestamp": datetime.utcnow().isoformat(),
                "is_final": not handoff_info.get("handoff_required", False),
                "summary": summary,
            }

            # Add handoff information if needed
            if handoff_info.get("handoff_required"):
                response.update(handoff_info)

            return response

        except Exception as e:
            import traceback

            self.logger.error(f"Analytics processing failed: {e}")
            self.logger.error(f"Traceback: {traceback.format_exc()}")
            self.status = AgentStatus.ERROR
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "is_final": True,
                "summary": f"Analytics processing failed: {str(e)}",
            }

    async def _handle_calculate_metrics(
        self, input_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Handle metrics calculation request."""
        time_range = input_data.get("time_range", "24h")
        metric_types = input_data.get("metric_types")

        metrics = self._calculate_metrics(time_range, metric_types)
        return {"metrics": metrics}

    async def _handle_generate_report(
        self, input_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Handle report generation request."""
        metrics = input_data.get("metrics")
        if not metrics:
            time_range = input_data.get("time_range", "24h")
            metrics = self._calculate_metrics(time_range)

        report_format = input_data.get("format", "markdown")
        report = self._generate_report(metrics, report_format)
        return {"report": report}

    async def _handle_forecast(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle forecasting request."""
        historical_data = input_data.get("historical_data")
        if not historical_data:
            # Fetch historical data
            time_range = input_data.get("time_range", "7d")
            historical_data = self._calculate_metrics(time_range)

        periods_ahead = input_data.get("periods_ahead", 7)
        forecast = self._forecast_performance(historical_data, periods_ahead)
        return {"forecast": forecast}

    async def _handle_anomaly_detection(
        self, input_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Handle anomaly detection request."""
        metrics_history = input_data.get("metrics_history", self.metrics_history)
        threshold = input_data.get("threshold", 2.0)

        anomalies = self._detect_anomalies(metrics_history, threshold)
        return {"anomalies": anomalies}

    async def _handle_ab_test(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle A/B test analysis request."""
        test_id = input_data.get("test_id", "unknown")
        variant_a = input_data.get("variant_a", {})
        variant_b = input_data.get("variant_b", {})

        analysis = self._analyze_ab_test(test_id, variant_a, variant_b)
        return {"ab_test_analysis": analysis}

    def _calculate_metrics(self, time_range: str, metric_types: list = None) -> dict:
        """
        Calculate comprehensive metrics from execution history.

        Args:
            time_range: Time range for analysis (e.g., '1h', '24h', '7d', '30d')
            metric_types: Optional list of metric types to calculate
                         ['campaign', 'agent', 'system'] (default: all)

        Returns:
            Dictionary containing:
            - campaign_metrics: Campaign performance metrics
            - agent_metrics: Agent performance metrics
            - system_metrics: System performance metrics
            - time_range: Time range analyzed
        """
        # Parse time range
        duration = self._parse_time_range(time_range)
        # Use timezone-aware datetime to match record timestamps
        cutoff_time = datetime.now(timezone.utc) - duration

        # Query execution history from memory manager
        execution_data = []
        if self.memory_manager:
            try:
                # Retrieve execution history from memory
                stored_history = self.memory_manager.retrieve(
                    agent_id="system", key="execution_history", memory_type="long_term"
                )
                if stored_history:
                    execution_data = stored_history
            except Exception as e:
                self.logger.warning(f"Could not retrieve execution history: {e}")

        # ALWAYS load synthetic data for demo/analytics - don't rely on sparse execution history
        # In production, this would be replaced with actual database queries
        if (
            len(execution_data) < 50
        ):  # If we have less than 50 records, supplement with synthetic
            try:
                self.logger.info("Loading synthetic data from files...")
                synthetic_data = load_execution_data(time_range=time_range)
                self.logger.info(
                    f"Loaded {len(synthetic_data)} records from synthetic data"
                )
                # Merge with any existing data (dedup by execution_id)
                existing_ids = {r.get("execution_id") for r in execution_data}
                for record in synthetic_data:
                    if record.get("execution_id") not in existing_ids:
                        execution_data.append(record)
            except Exception as e:
                self.logger.error(f"Failed to load synthetic data: {e}")
                # Fall back to agent's local execution history if synthetic load fails
                if not execution_data:
                    execution_data = self.execution_history

        # Filter by time range
        filtered_data = [
            record
            for record in execution_data
            if self._is_within_time_range(record, cutoff_time)
        ]

        # If no data found in time range, use all available data
        # This ensures we always have metrics to show users
        if len(filtered_data) == 0 and len(execution_data) > 0:
            self.logger.info(
                f"No data found in {time_range} range, using all {len(execution_data)} available records"
            )
            filtered_data = execution_data
            # Update time range to reflect actual data used
            if execution_data:
                dates = []
                for record in execution_data:
                    try:
                        started = record.get("started_at", "")
                        if isinstance(started, str):
                            dates.append(
                                datetime.fromisoformat(started.replace("Z", "+00:00"))
                            )
                    except:
                        continue
                if dates:
                    cutoff_time = min(dates)
                    time_range = "all_available"

        # Determine which metrics to calculate
        if metric_types is None:
            metric_types = ["campaign", "agent", "system"]

        metrics = {
            "time_range": time_range,
            "data_points": len(filtered_data),
            "period_start": cutoff_time.isoformat(),
            "period_end": datetime.now(timezone.utc).isoformat(),
        }

        # Calculate requested metrics
        if "campaign" in metric_types:
            # Filter for records with campaign metrics (impressions, clicks, etc.)
            campaign_records = [
                record
                for record in filtered_data
                if record.get("result", {}).get("metrics", {}).get("impressions")
                is not None
            ]
            self.logger.info(
                f"Filtering for campaign metrics: {len(campaign_records)}/{len(filtered_data)} records have campaign data"
            )
            metrics["campaign_metrics"] = calculate_campaign_metrics(campaign_records)

        if "agent" in metric_types:
            metrics["agent_metrics"] = calculate_agent_metrics(filtered_data)

        if "system" in metric_types:
            metrics["system_metrics"] = calculate_system_metrics(filtered_data)

        # Store in history for trend analysis
        self.metrics_history.append(
            {
                "timestamp": datetime.utcnow().isoformat(),
                "metrics": metrics,
            }
        )

        # Keep only recent history (last 100 records)
        if len(self.metrics_history) > 100:
            self.metrics_history = self.metrics_history[-100:]

        return metrics

    def _generate_report(self, metrics: dict, format: str = "markdown") -> dict:
        """
        Generate structured analytics report.

        Args:
            metrics: Metrics dictionary from _calculate_metrics
            format: Report format ('markdown', 'json', 'html')

        Returns:
            Dictionary containing:
            - report_content: Formatted report string
            - visualizations: List of chart configurations
            - insights: List of key insights
            - format: Report format
        """
        campaign_metrics = metrics.get("campaign_metrics", {})
        agent_metrics = metrics.get("agent_metrics", {})
        system_metrics = metrics.get("system_metrics", {})

        # Generate insights
        insights = self._generate_insights(metrics)

        # Generate visualizations
        visualizations = []
        visualizations.extend(generate_campaign_dashboard(metrics))
        visualizations.extend(generate_agent_dashboard(metrics))
        visualizations.extend(generate_system_dashboard(metrics))

        # Generate report content based on format
        if format == "markdown":
            report_content = self._generate_markdown_report(metrics, insights)
        elif format == "json":
            report_content = {
                "metrics": metrics,
                "insights": insights,
            }
        elif format == "html":
            report_content = self._generate_html_report(metrics, insights)
        else:
            report_content = str(metrics)

        # Get comparison with previous period if available
        comparison = None
        if len(self.metrics_history) >= 2:
            try:
                previous_metrics = self.metrics_history[-2]["metrics"]
                comparison = self._compare_periods(metrics, previous_metrics)
            except Exception as e:
                self.logger.warning(f"Could not generate comparison: {e}")

        return {
            "report_content": report_content,
            "visualizations": visualizations,
            "insights": insights,
            "format": format,
            "comparison": comparison,
            "generated_at": datetime.utcnow().isoformat(),
        }

    def _forecast_performance(
        self, historical_data: dict, periods_ahead: int = 7
    ) -> dict:
        """
        Forecast future performance using time series analysis.

        Args:
            historical_data: Historical metrics data
            periods_ahead: Number of periods to forecast

        Returns:
            Dictionary containing:
            - forecasts: Predicted values for each metric
            - confidence_intervals: Upper and lower bounds
            - method: Forecasting method used
        """
        forecasts = {}
        confidence_intervals = {}

        # Extract time series data from metrics history
        if not self.metrics_history or len(self.metrics_history) < 3:
            # Not enough data for forecasting
            return {
                "forecasts": {},
                "confidence_intervals": {},
                "method": "insufficient_data",
                "message": "Need at least 3 historical data points for forecasting",
            }

        # Key metrics to forecast
        metrics_to_forecast = [
            ("campaign_metrics", "ctr"),
            ("campaign_metrics", "conversion_rate"),
            ("campaign_metrics", "roi"),
            ("agent_metrics", "success_rate"),
            ("agent_metrics", "avg_response_time"),
            ("system_metrics", "throughput"),
        ]

        for category, metric_name in metrics_to_forecast:
            try:
                # Extract historical values
                values = []
                for record in self.metrics_history[-30:]:  # Last 30 periods
                    metric_category = record.get("metrics", {}).get(category, {})
                    value = metric_category.get(metric_name)
                    if value is not None:
                        values.append(float(value))

                if len(values) < 3:
                    continue

                # Use simple moving average for forecasting
                window_size = min(7, len(values))
                forecast_value = statistics.mean(values[-window_size:])

                # Calculate standard deviation for confidence intervals
                std_dev = statistics.stdev(values) if len(values) > 1 else 0

                key = f"{category}.{metric_name}"
                forecasts[key] = round(forecast_value, 2)
                confidence_intervals[key] = {
                    "lower": round(forecast_value - 1.96 * std_dev, 2),
                    "upper": round(forecast_value + 1.96 * std_dev, 2),
                }

            except Exception as e:
                self.logger.warning(f"Could not forecast {category}.{metric_name}: {e}")

        return {
            "forecasts": forecasts,
            "confidence_intervals": confidence_intervals,
            "method": "moving_average",
            "periods_ahead": periods_ahead,
            "forecast_date": (
                datetime.utcnow() + timedelta(days=periods_ahead)
            ).isoformat(),
        }

    def _detect_anomalies(self, metrics_history: list, threshold: float = 2.0) -> dict:
        """
        Detect anomalies in metrics using statistical methods.

        Args:
            metrics_history: List of historical metrics
            threshold: Z-score threshold for anomaly detection (default: 2.0)

        Returns:
            Dictionary containing:
            - anomalies: List of detected anomalies
            - severity_counts: Count by severity level
            - alerts: List of actionable alerts
        """
        if not metrics_history or len(metrics_history) < 10:
            return {
                "anomalies": [],
                "severity_counts": {"warning": 0, "critical": 0},
                "alerts": [],
                "message": "Insufficient data for anomaly detection (need at least 10 data points)",
            }

        anomalies = []
        alerts = []

        # Key metrics to monitor
        metrics_to_monitor = [
            ("campaign_metrics", "ctr", "lower_is_bad"),
            ("campaign_metrics", "conversion_rate", "lower_is_bad"),
            ("campaign_metrics", "roi", "lower_is_bad"),
            ("agent_metrics", "success_rate", "lower_is_bad"),
            ("agent_metrics", "error_rate", "higher_is_bad"),
            ("system_metrics", "error_rate", "higher_is_bad"),
            ("system_metrics", "latency_p99", "higher_is_bad"),
        ]

        for category, metric_name, direction in metrics_to_monitor:
            try:
                # Extract values
                values = []
                timestamps = []
                for record in metrics_history:
                    metrics = record.get("metrics", {})
                    metric_category = metrics.get(category, {})
                    value = metric_category.get(metric_name)
                    if value is not None:
                        values.append(float(value))
                        timestamps.append(record.get("timestamp", ""))

                if len(values) < 10:
                    continue

                # Calculate z-scores
                mean_value = statistics.mean(values)
                std_dev = statistics.stdev(values) if len(values) > 1 else 0

                if std_dev == 0:
                    continue

                # Check last value for anomaly
                last_value = values[-1]
                z_score = (last_value - mean_value) / std_dev

                # Determine if anomalous based on direction
                is_anomaly = False
                severity = None

                if direction == "lower_is_bad" and z_score < -threshold:
                    is_anomaly = True
                    severity = "critical" if z_score < -3.0 else "warning"
                elif direction == "higher_is_bad" and z_score > threshold:
                    is_anomaly = True
                    severity = "critical" if z_score > 3.0 else "warning"

                if is_anomaly:
                    anomaly_record = {
                        "metric": f"{category}.{metric_name}",
                        "value": last_value,
                        "z_score": round(z_score, 2),
                        "mean": round(mean_value, 2),
                        "std_dev": round(std_dev, 2),
                        "severity": severity,
                        "timestamp": timestamps[-1] if timestamps else "",
                        "direction": direction,
                    }
                    anomalies.append(anomaly_record)

                    # Generate alert
                    alert_msg = (
                        f"{severity.upper()}: {metric_name} is {'below' if direction == 'lower_is_bad' else 'above'} "
                        f"normal range. Current: {last_value:.2f}, Expected: {mean_value:.2f} Â± {std_dev:.2f}"
                    )
                    alerts.append(
                        {
                            "severity": severity,
                            "message": alert_msg,
                            "metric": f"{category}.{metric_name}",
                        }
                    )

            except Exception as e:
                self.logger.warning(
                    f"Could not detect anomalies for {category}.{metric_name}: {e}"
                )

        # Count by severity
        severity_counts = {
            "warning": sum(1 for a in anomalies if a["severity"] == "warning"),
            "critical": sum(1 for a in anomalies if a["severity"] == "critical"),
        }

        return {
            "anomalies": anomalies,
            "severity_counts": severity_counts,
            "alerts": alerts,
            "threshold": threshold,
            "analyzed_at": datetime.utcnow().isoformat(),
        }

    def _analyze_ab_test(self, test_id: str, variant_a: dict, variant_b: dict) -> dict:
        """
        Analyze A/B test results for statistical significance.

        Args:
            test_id: Unique test identifier
            variant_a: Metrics for variant A (control)
            variant_b: Metrics for variant B (treatment)

        Returns:
            Dictionary containing:
            - winner: 'A', 'B', or 'no_significant_difference'
            - confidence: Confidence level (0-1)
            - p_value: Statistical p-value
            - recommendation: Actionable recommendation
        """
        try:
            # Extract conversion data
            conversions_a = variant_a.get("conversions", 0)
            trials_a = variant_a.get("trials", variant_a.get("impressions", 0))

            conversions_b = variant_b.get("conversions", 0)
            trials_b = variant_b.get("trials", variant_b.get("impressions", 0))

            if trials_a == 0 or trials_b == 0:
                return {
                    "test_id": test_id,
                    "winner": "insufficient_data",
                    "confidence": 0.0,
                    "p_value": 1.0,
                    "recommendation": "Collect more data before making a decision",
                    "error": "Insufficient sample size",
                }

            # Calculate conversion rates
            rate_a = conversions_a / trials_a
            rate_b = conversions_b / trials_b

            # Perform chi-square test
            observed = np.array(
                [
                    [conversions_a, trials_a - conversions_a],
                    [conversions_b, trials_b - conversions_b],
                ]
            )

            chi2, p_value, dof, expected = stats.chi2_contingency(observed)

            # Determine winner
            alpha = 0.05  # 95% confidence level
            confidence = 1 - p_value

            if p_value < alpha:
                winner = "B" if rate_b > rate_a else "A"
                is_significant = True
            else:
                winner = "no_significant_difference"
                is_significant = False

            # Calculate lift
            lift = ((rate_b - rate_a) / rate_a * 100) if rate_a > 0 else 0

            # Generate recommendation
            if is_significant and winner == "B":
                recommendation = (
                    f"Implement Variant B. It shows a {abs(lift):.2f}% "
                    f"{'increase' if lift > 0 else 'decrease'} in conversion rate with "
                    f"{confidence*100:.1f}% confidence."
                )
            elif is_significant and winner == "A":
                recommendation = (
                    f"Keep Variant A (control). Variant B shows a {abs(lift):.2f}% "
                    f"decrease in conversion rate with {confidence*100:.1f}% confidence."
                )
            else:
                recommendation = (
                    "Continue testing. No statistically significant difference detected. "
                    f"Current p-value: {p_value:.4f}. Need p-value < {alpha}."
                )

            return {
                "test_id": test_id,
                "winner": winner,
                "confidence": round(confidence, 4),
                "p_value": round(p_value, 4),
                "is_significant": is_significant,
                "recommendation": recommendation,
                "metrics": {
                    "variant_a": {
                        "conversion_rate": round(rate_a * 100, 2),
                        "conversions": conversions_a,
                        "trials": trials_a,
                    },
                    "variant_b": {
                        "conversion_rate": round(rate_b * 100, 2),
                        "conversions": conversions_b,
                        "trials": trials_b,
                    },
                    "lift": round(lift, 2),
                },
                "statistical_test": "chi_square",
                "alpha": alpha,
            }

        except Exception as e:
            self.logger.error(f"A/B test analysis failed: {e}")
            return {
                "test_id": test_id,
                "winner": "error",
                "confidence": 0.0,
                "p_value": 1.0,
                "recommendation": "Error occurred during analysis",
                "error": str(e),
            }

    # Helper methods

    def _parse_time_range(self, time_range: str) -> timedelta:
        """Parse time range string to timedelta."""
        # Handle special case for "all_available"
        if time_range == "all_available":
            return timedelta(days=3650)  # 10 years - effectively all data

        if not time_range or len(time_range) < 2:
            return timedelta(hours=24)  # Default to 24 hours

        try:
            unit = time_range[-1]
            value = int(time_range[:-1])

            if unit == "h":
                return timedelta(hours=value)
            elif unit == "d":
                return timedelta(days=value)
            elif unit == "w":
                return timedelta(weeks=value)
            else:
                return timedelta(hours=24)  # Default to 24 hours
        except (ValueError, IndexError):
            return timedelta(hours=24)  # Default to 24 hours

    def _is_within_time_range(self, record: dict, cutoff_time: datetime) -> bool:
        """Check if record is within time range."""
        timestamp = record.get("started_at")
        if not timestamp:
            return False

        if isinstance(timestamp, str):
            timestamp = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))

        # Ensure both datetimes are timezone-aware for comparison
        if timestamp.tzinfo is None:
            timestamp = timestamp.replace(tzinfo=timezone.utc)
        if cutoff_time.tzinfo is None:
            cutoff_time = cutoff_time.replace(tzinfo=timezone.utc)

        return timestamp >= cutoff_time

    def _generate_insights(self, metrics: dict) -> List[str]:
        """Generate actionable insights from metrics."""
        insights = []

        campaign = metrics.get("campaign_metrics", {})
        agent = metrics.get("agent_metrics", {})
        system = metrics.get("system_metrics", {})

        # Campaign insights
        if campaign.get("ctr", 0) < 2.0:
            insights.append(
                "CTR is below industry average (2%). Consider improving ad copy or targeting."
            )

        if campaign.get("conversion_rate", 0) < 2.0:
            insights.append("Conversion rate is low. Optimize landing page and CTA.")

        if campaign.get("roi", 0) > 200:
            insights.append("Excellent ROI! Consider scaling this campaign.")

        # Agent insights
        if agent.get("error_rate", 0) > 5:
            insights.append(
                "Agent error rate is elevated. Review recent changes and logs."
            )

        if agent.get("success_rate", 0) > 95:
            insights.append("Agents are performing excellently with high success rate.")

        # System insights
        if system.get("latency_p99", 0) > 5.0:
            insights.append("P99 latency is high. Investigate performance bottlenecks.")

        return insights

    async def _detect_handoff_need(
        self, result: Dict[str, Any], input_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Detect if analysis results warrant a handoff to another agent using LLM reasoning.

        Args:
            result: The analysis result dictionary
            input_data: The original request data

        Returns:
            Dictionary with handoff information if needed, empty dict otherwise
        """
        try:
            user_message = input_data.get("message", "")

            # Use LLM-driven handoff detection
            handoff_info = await self.handoff_detector.detect_handoff(
                current_agent="analytics_evaluation",
                user_message=user_message,
                agent_analysis=result,
            )

            return handoff_info

        except Exception as e:
            self.logger.error(f"Handoff detection failed: {e}", exc_info=True)
            return {}

    def _generate_contextual_report(self, metrics: dict, user_query: str) -> str:
        """Generate contextual report based on user query."""
        campaign = metrics.get("campaign_metrics", {})

        # Extract key metrics
        roi = campaign.get("roi", 0)
        ctr = campaign.get("ctr", 0)
        conversion_rate = campaign.get("conversion_rate", 0)
        impressions = campaign.get("total_impressions", 0)
        conversions = campaign.get("total_conversions", 0)

        # Create contextual response based on query keywords
        query_lower = user_query.lower()

        # Handle mobile vs desktop comparison queries
        if any(word in query_lower for word in ["mobile", "desktop", "device"]) and any(
            word in query_lower for word in ["vs", "versus", "compare", "comparison"]
        ):
            report = f"""# Mobile vs Desktop Performance Analysis

Based on your query: "{user_query}"

## Device Performance Overview
- **Overall Conversion Rate**: {conversion_rate:.2f}%
- **Total Conversions**: {conversions:,}
- **Average ROI**: {roi:.2f}%

## Key Insights
The data shows campaigns are running across multiple devices. Here's what we observe:

### Mobile Performance
- Mobile campaigns typically show higher engagement rates
- Conversion rates vary by campaign type and channel
- Mobile-first design is crucial for optimization

### Desktop Performance
- Desktop users often have higher conversion values
- Better for complex purchasing decisions
- Longer session durations typical

## Recommendations
1. **Optimize mobile experience**: Ensure fast loading and simple checkout
2. **Test device-specific creatives**: Tailor messaging for each platform
3. **Track device attribution**: Understand cross-device customer journeys
4. **Allocate budget strategically**: Invest based on device performance
5. **A/B test by device**: Run separate experiments for mobile and desktop

## Channel Breakdown
Analysis shows performance varies by channel. Consider running device-specific campaigns for better results.
"""

        # Handle traffic trend queries
        elif any(
            word in query_lower
            for word in ["trend", "trended", "trending", "over time", "past"]
        ) and any(word in query_lower for word in ["traffic", "visits", "visitors"]):
            report = f"""# Website Traffic Trend Analysis

Based on your query: "{user_query}"

## Traffic Overview
- **Total Impressions**: {impressions:,}
- **Click-Through Rate**: {ctr:.2f}%
- **Engagement**: {impressions:,} impressions tracked

## Trend Analysis
Based on available data, we can observe:

### Key Findings
- Traffic patterns show variation across different periods
- Seasonal factors may influence visitor behavior
- Channel mix impacts overall traffic volume

### Performance Indicators
- **Current CTR**: {ctr:.2f}% {"(Strong performance)" if ctr > 2.5 else "(Moderate performance)" if ctr > 1.5 else "(Needs improvement)"}
- **Conversion Rate**: {conversion_rate:.2f}%
- **Total Conversions**: {conversions:,}

## Recommendations
1. **Monitor seasonal patterns**: Track traffic by week/month to identify trends
2. **Optimize high-traffic periods**: Increase budget during peak times
3. **Improve conversion during low traffic**: Focus on quality over quantity
4. **Diversify traffic sources**: Reduce dependency on single channels
5. **Set up automated alerts**: Get notified of significant traffic changes

## Next Steps
Consider implementing time-series analysis for more detailed trend forecasting.
"""

        # Handle channel comparison queries
        elif any(
            word in query_lower for word in ["compare", "comparison", "vs", "versus"]
        ) and any(
            word in query_lower
            for word in ["channel", "email", "social", "search", "paid"]
        ):
            report = f"""# Channel Performance Comparison

Based on your query: "{user_query}"

## Overall Channel Performance
- **Total Campaigns**: {conversions:,} conversions tracked
- **Average ROI**: {roi:.2f}%
- **Overall CTR**: {ctr:.2f}%

## Channel Analysis

### Email Marketing
- Typically shows high engagement with existing customers
- Strong for retention and repeat purchases
- Cost-effective channel with good ROI

### Social Media
- Great for brand awareness and engagement
- Varies by platform (LinkedIn, Facebook, Instagram)
- Important for reaching new audiences

### Paid Search
- High-intent traffic with strong conversion potential
- Requires continuous optimization and budget management
- Scalable for growth when profitable

## Comparative Insights
- **ROI Leader**: Channels with {roi:.2f}% average return
- **Volume Leader**: Driving {impressions:,} impressions
- **Conversion Leader**: Achieving {conversion_rate:.2f}% conversion rate

## Recommendations
1. **Double down on winners**: Increase budget for top-performing channels
2. **Optimize underperformers**: Test new approaches or reduce spend
3. **Cross-channel attribution**: Understand how channels work together
4. **Test incrementally**: Run controlled experiments to validate changes
5. **Monitor competition**: Track competitor activity in each channel

## Strategic Considerations
Each channel serves different purposes in the customer journey. Balance brand awareness, consideration, and conversion objectives.
"""

        # Handle funnel analysis queries
        elif any(
            word in query_lower
            for word in [
                "funnel",
                "conversion funnel",
                "checkout",
                "drop-off",
                "abandonment",
            ]
        ):
            report = f"""# Conversion Funnel Analysis

Based on your query: "{user_query}"

## Funnel Overview
- **Top of Funnel**: {impressions:,} impressions
- **Middle of Funnel**: {int(impressions * (ctr/100)):,} clicks (CTR: {ctr:.2f}%)
- **Bottom of Funnel**: {conversions:,} conversions (Conversion Rate: {conversion_rate:.2f}%)

## Stage-by-Stage Analysis

### 1. Awareness Stage
- **Impressions**: {impressions:,}
- **Reach**: Strong visibility across channels

### 2. Consideration Stage
- **Clicks**: {int(impressions * (ctr/100)):,}
- **Engagement Rate**: {ctr:.2f}%
- {"âœ“ Good engagement" if ctr > 2.5 else "âš ï¸ Could be improved"}

### 3. Conversion Stage
- **Conversions**: {conversions:,}
- **Conversion Rate**: {conversion_rate:.2f}%
- {"âœ“ Strong conversion" if conversion_rate > 3 else "âš ï¸ Optimization opportunity"}

## Drop-off Analysis
- **Impression to Click**: {100 - ctr:.1f}% drop-off
- **Click to Conversion**: {100 - (conversion_rate * ctr / 100):.1f}% drop-off

## Optimization Recommendations
1. **Reduce friction**: Simplify the checkout/conversion process
2. **Improve messaging**: Ensure consistency from ad to landing page
3. **Add trust signals**: Include reviews, guarantees, security badges
4. **Optimize load speed**: Every second counts for conversions
5. **A/B test CTAs**: Experiment with different call-to-action copy and design
6. **Implement retargeting**: Re-engage users who dropped off
7. **Analyze exit pages**: Identify where users are leaving

## ROI Impact
With {conversions:,} conversions and {roi:.2f}% ROI, {"focus on scaling successful campaigns" if roi > 200 else "prioritize conversion rate optimization"}.
"""

        # Handle prediction/forecast accuracy queries
        elif any(
            word in query_lower
            for word in [
                "prediction",
                "predictions",
                "forecast",
                "accurate",
                "accuracy",
            ]
        ):
            # Check if asking about conversion rates specifically
            if "conversion" in query_lower:
                report = f"""# Conversion Rate Prediction Analysis

Based on your query: "{user_query}"

## Current Conversion Performance
- **Actual Conversion Rate**: {conversion_rate:.2f}%
- **Total Conversions**: {conversions:,}
- **Sample Size**: {impressions:,} impressions

## Prediction Accuracy Assessment
{"âœ“ With " + f"{impressions:,}" + " impressions, prediction confidence is high." if impressions > 10000 else "âš ï¸ Sample size is limited. More data needed for reliable predictions."}

## Historical Performance
- Current conversion rate: {conversion_rate:.2f}%
- Expected range: {max(0, conversion_rate - 0.5):.2f}% - {conversion_rate + 0.5:.2f}%

## Recommendations for Improving Predictions
1. **Increase data collection**: Gather more conversion data across different segments
2. **Track seasonal patterns**: Monitor conversion trends over time
3. **Segment analysis**: Break down conversions by channel, audience, and campaign type
4. **A/B testing**: Run controlled experiments to validate prediction models
5. **Update baselines**: Refresh prediction models quarterly with recent data

## Model Confidence
{"High confidence" if impressions > 10000 and conversions > 100 else "Medium confidence" if impressions > 5000 else "Low confidence - more data needed"}
"""
            else:
                # General prediction/forecast query
                report = f"""# Performance Prediction Analysis

Based on your query: "{user_query}"

## Current Metrics (Actual)
- **ROI**: {roi:.2f}%
- **CTR**: {ctr:.2f}%
- **Conversion Rate**: {conversion_rate:.2f}%

## Prediction Reliability
Based on {impressions:,} impressions and {conversions:,} conversions:
- Confidence Level: {"High (>95%)" if impressions > 10000 else "Medium (80-95%)" if impressions > 5000 else "Low (<80%)"}
- Data Quality: {"Good" if conversions > 100 else "Limited"}

## Improving Forecast Accuracy
1. Collect more historical data across multiple campaigns
2. Implement time-series analysis for trend detection
3. Account for external factors (seasonality, market conditions)
4. Use ensemble models combining multiple prediction methods
5. Regularly validate predictions against actual results
"""
        elif any(word in query_lower for word in ["roi", "return"]):
            report = f"""# ROI Analysis

Based on your query: "{user_query}"

## Current ROI Performance
- **ROI**: {roi:.2f}%
- **Total Conversions**: {conversions:,}
- **Conversion Rate**: {conversion_rate:.2f}%

## Analysis
{"Your ROI is strong at " + str(round(roi, 1)) + "%, indicating efficient campaign spend." if roi > 200 else "ROI is below target. Consider optimizing your campaigns for better returns."}

## Key Metrics
- Impressions: {impressions:,}
- CTR: {ctr:.2f}%
"""
        elif any(word in query_lower for word in ["ctr", "click", "engagement"]):
            report = f"""# Click-Through Rate Analysis

Based on your query: "{user_query}"

## Current CTR Performance
- **CTR**: {ctr:.2f}%
- **Total Impressions**: {impressions:,}
- **Engagement Level**: {"High" if ctr > 2.5 else "Moderate" if ctr > 1.5 else "Needs Improvement"}

## Analysis
{"Excellent engagement! Your CTR is above industry average." if ctr > 2.5 else "CTR is moderate. Consider testing new ad creatives or targeting." if ctr > 1.5 else "CTR is below average. Immediate optimization recommended."}
"""
        elif any(word in query_lower for word in ["conversion", "converting"]):
            report = f"""# Conversion Analysis

Based on your query: "{user_query}"

## Current Conversion Performance
- **Conversion Rate**: {conversion_rate:.2f}%
- **Total Conversions**: {conversions:,}
- **CTR**: {ctr:.2f}%

## Analysis
{"Strong conversion rate! Your campaigns are effectively driving actions." if conversion_rate > 3 else "Conversion rate is moderate. Optimize landing pages and CTAs." if conversion_rate > 2 else "Low conversion rate. Review user journey and remove friction points."}
"""
        elif any(
            word in query_lower for word in ["performance", "metrics", "campaign"]
        ):
            report = f"""# Campaign Performance Overview

Based on your query: "{user_query}"

## Overall Performance
- **ROI**: {roi:.2f}%
- **CTR**: {ctr:.2f}%
- **Conversion Rate**: {conversion_rate:.2f}%
- **Total Impressions**: {impressions:,}
- **Total Conversions**: {conversions:,}

## Performance Summary
{self._get_performance_summary(roi, ctr, conversion_rate)}
"""
        else:
            # Generic response for other queries
            report = f"""# Analytics Report

Based on your query: "{user_query}"

## Key Metrics
- **ROI**: {roi:.2f}%
- **CTR**: {ctr:.2f}%
- **Conversion Rate**: {conversion_rate:.2f}%
- **Impressions**: {impressions:,}
- **Conversions**: {conversions:,}

## Overview
Your campaigns are currently {'performing well' if roi > 200 else 'showing moderate results' if roi > 150 else 'underperforming'}.
"""

        return report

    def _get_performance_summary(
        self, roi: float, ctr: float, conversion_rate: float
    ) -> str:
        """Get overall performance summary based on metrics."""
        if roi > 250 and ctr > 3 and conversion_rate > 3:
            return "ðŸŒŸ Excellent! All metrics are performing above expectations. Consider scaling these campaigns."
        elif roi > 200 or (ctr > 2.5 and conversion_rate > 2.5):
            return "âœ“ Good performance. Some metrics are strong, but there's room for optimization."
        elif roi > 150:
            return "âš ï¸ Moderate performance. Review campaigns and identify improvement opportunities."
        else:
            return "âš ï¸ Performance is below targets. Consider strategic review and optimization."

    def _generate_markdown_report(self, metrics: dict, insights: List[str]) -> str:
        """Generate markdown-formatted report."""
        campaign = metrics.get("campaign_metrics", {})
        agent = metrics.get("agent_metrics", {})
        system = metrics.get("system_metrics", {})

        report = f"""# Analytics Report
Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}
Time Range: {metrics.get('time_range', 'N/A')}

## Executive Summary

### Campaign Performance
- **CTR**: {campaign.get('ctr', 0):.2f}%
- **Conversion Rate**: {campaign.get('conversion_rate', 0):.2f}%
- **ROI**: {campaign.get('roi', 0):.2f}%
- **Total Impressions**: {campaign.get('total_impressions', 0):,}
- **Total Conversions**: {campaign.get('total_conversions', 0):,}

### Agent Performance
- **Success Rate**: {agent.get('success_rate', 0):.2f}%
- **Avg Response Time**: {agent.get('avg_response_time', 0):.2f}s
- **Total Executions**: {agent.get('total_executions', 0):,}

### System Performance
- **Throughput**: {system.get('throughput', 0):.2f} executions/hour
- **Error Rate**: {system.get('error_rate', 0):.2f}%
- **P99 Latency**: {system.get('latency_p99', 0):.3f}s

## Key Insights

"""
        for i, insight in enumerate(insights, 1):
            report += f"{i}. {insight}\n"

        report += "\n## Recommendations\n\n"
        report += "- Monitor trends and adjust strategies accordingly\n"
        report += "- Focus on metrics showing declining performance\n"
        report += "- Scale successful campaigns and optimize underperforming ones\n"

        return report

    def _generate_html_report(self, metrics: dict, insights: List[str]) -> str:
        """Generate HTML-formatted report."""
        # Simple HTML wrapper around markdown
        markdown_content = self._generate_markdown_report(metrics, insights)
        html = f"""
        <html>
        <head><title>Analytics Report</title></head>
        <body>
        <pre>{markdown_content}</pre>
        </body>
        </html>
        """
        return html

    def _compare_periods(self, current: dict, previous: dict) -> dict:
        """Compare current period with previous period."""
        comparison = {}

        # Helper function
        def calc_change(curr_val, prev_val):
            if prev_val == 0:
                return 100.0 if curr_val > 0 else 0.0
            return (curr_val - prev_val) / prev_val * 100

        # Campaign comparisons
        curr_campaign = current.get("campaign_metrics", {})
        prev_campaign = previous.get("campaign_metrics", {})

        comparison["campaign"] = {
            "ctr_change": calc_change(
                curr_campaign.get("ctr", 0), prev_campaign.get("ctr", 0)
            ),
            "conversion_rate_change": calc_change(
                curr_campaign.get("conversion_rate", 0),
                prev_campaign.get("conversion_rate", 0),
            ),
            "roi_change": calc_change(
                curr_campaign.get("roi", 0), prev_campaign.get("roi", 0)
            ),
        }

        return comparison
